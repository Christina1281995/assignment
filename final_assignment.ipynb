{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the final assignment: \n",
    "\n",
    "In small groups, you are going to perform a full front to end geospatial and semantic analysis of social media data. This is a task that has many real-world application contexts and all of the steps you will complete in this assignment can be useful as a point of reference later on in project work. \n",
    "\n",
    "- First, you will access CSV data and do some <b>preprocessing</b> steps to prepare the data for geospatial and semantic analyses.\n",
    "- You'll then <b>decide</b> on which model from the hugging face platform is appropriate for your semantic analyses. \n",
    "- Using the <b>pipeline</b> approach, you'll generate a new column with your analysis results.\n",
    "- After that, you will perform a <b>geospatial analysis</b> on the results of your <b>semantic analysis</b>. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with any kind of geospatial analysis of textual data, you will first need to import the data and prepare it for any further analyses. These steps are very common and you will likely have to perform these steps any time you want to do a similar analysis on your own or with different data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0. Import Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages here\n",
    "#import tensorflow as tf\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "import pandas as pd                                                    # for data handling\n",
    "import xml.etree.cElementTree as ET                                    # for parsing XML file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import mapclassify                                                     # required for animated scatterplot map (plotly)\n",
    "import geopandas as gpd                                                # geographic data handling\n",
    "import folium                                                          # interactive mapping capabilities\n",
    "import folium.plugins as plugins\n",
    "import plpygis                                                         # a converter to and from the PostGIS geometry types, WKB, GeoJSON and Shapely formats\n",
    "from plpygis import Geometry\n",
    "from shapely.geometry import Point, Polygon, shape                     # creating geospatial data\n",
    "from shapely import wkb, wkt                                           # creating and parsing geospatial data\n",
    "import shapely                                                  \n",
    "\n",
    "import plotly\n",
    "import plotly.express as px                                            # for interactive, animated timeseries map\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "# import json\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "from wordcloud import STOPWORDS\n",
    "from PIL import Image    # for masking wordcloud with an image\n",
    "import requests          # for accessing url image\n",
    "from io import BytesIO   # for accedssing url image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Get the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways you can get data for analysis. Twitter data can be publicly accessed .... . In this case we will access a CSV file that already contain a bunch of tweets. Download the data from https://drive.google.com/file/d/1hHkde7xSGNlemYRw6yFFj5RfRfAGbFHS/view?usp=share_link and add it to this repository."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Load the data into a pandas DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data using the pandas function .read_csv()\n",
    "\n",
    "Hint: you may have to specify the correct encoding of csv so that the pandas library can read the data properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Load the CSV data in to a pandas dataframe called \"data\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Take a first look at the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check how large the dataset is by printing out the number of rows in the dataset\n",
    "- Take a look at what kind of columns the dataset includes by printing out the names of the columns \n",
    "- Check the data types by printing out the individual data types of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Check how large the dataset is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Check what columns are included in the dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Check the datatypes for each of the columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Create a geodataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a crucial part of the data preparation: we need to be able to work with the spatial characteristics of this data. As you will have seen in the data types, the \"geom\" column is an 'object' type. To do any geospatial analyses we need to convert this DataFrame into a GeoDataFrame. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at what the \"geom\" column looks like. You can use the ```.loc``` method to display the \"geom\" value of the first row, just to get an idea of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Display the \"geom\" value in the first row of the dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a representation of a polygon using the <b>Well-Known Text (WKT)</b> format. WKT is a text-based representation of geometry objects, defined by the Open Geospatial Consortium (OGC)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two key steps involved in the conversion of this text-based representation to an actual geometry data type:\n",
    "1. Convert the WKT strings into shapely ```'Polygon'``` objects. This is done by using the shapely ```.apply(wkt.loads)``` function on the \"geom\" column.\n",
    "2. Use the ```GeoDataFrame``` constructor from the ```geopandas``` library to create a geodataframe. Make sure to specify the coordinate reference system as \"EPSG:4326\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: 1. Create a new column in the dataframe called \"geometry\" by converting the 'geom' column to shapely Polygon objects \n",
    "\n",
    "# TO DO: 2. Create a geodataframe called \"geodata\" from the pandas \"data\" dataframe, make sure you add the criteria crs='EPSG:4326'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what kind of geodatatypes we have. This code is prepared for you: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE FOR YOU: Check the geodatatypes in the dataset\n",
    "# Delete the three ''' to use the code below\n",
    "\n",
    "'''\n",
    "print(geodata.geom_type.value_counts())\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Complete this markdown cell with your answers:</span>\n",
    "\n",
    "1. Describe what the geometry types of your dataset are: \n",
    "\n",
    "2. Why do you think they have those geometry types (think about how the data might be collected from Twitter)?\n",
    "\n",
    "3. Why might point geometries be more useful compared to polygon geometries when it comes to spatial analyses? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we only have points. One way of doing this is by turning all polygons into points, where the new point is the polygon's center. \n",
    "\n",
    "To do that we first need to identify all rows that are polygons. This can be done by creating a new column called ```\"geom_type\"``` in our \"geodata\" geodataframe. We can create the column by applying a ```lambda``` function to each row, where the function just checks the ```geom_type``` of each row based on the ```\"geometry\"``` column. This bit of code is prepared for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE FOR YOU: Create a new column called \"geom_type\"\n",
    "# Delete the three ''' to use the code below\n",
    "\n",
    "'''\n",
    "geodata['geom_type'] = geodata['geometry'].apply(lambda x: x.geom_type)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can iterate over each row in the ```geodata``` DataFrame to check if each row represents a polygon geometry, and if so, replace the polygon geometry with a point geometry representing the centroid of the polygon. This bit of code is prepared for you as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE FOR YOU: Iterate over every row in the dataframe to replace all polygon geometries with point geometries\n",
    "# Delete the three ''' to use the code below\n",
    "\n",
    "'''\n",
    "# Iterate over each row in the geodataframe\n",
    "for index, row in geodata.iterrows():\n",
    "    # Check if the row is a Polygon\n",
    "    if row['geom_type'] == 'Polygon':\n",
    "        # Get the centroid of the polygon\n",
    "        centroid = row['geometry'].centroid\n",
    "        # Create a new Point geometry from the centroid\n",
    "        point_geom = Point(centroid.x, centroid.y)\n",
    "        # Update the geometry of the row with the new Point geometry\n",
    "        geodata.at[index, 'geometry'] = point_geom\n",
    "        # Update the geom_type of the row to 'Point'\n",
    "        geodata.at[index, 'geom_type'] = 'Point'\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's just make sure we have successfully converted all our geometries into points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE FOR YOU: Check the geometry types again to make sure we successfully converted all polygons\n",
    "# Delete the three ''' to use the code below\n",
    "\n",
    "'''\n",
    "print(geodata.geom_type.value_counts())\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Filter the data by relevant columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 3, when you checked the columns in this dataset, you will have noticed that there are quite a few columns. It's unlikely that you'll require all of those for your analysis. Therefore take a moment to create a new DataFrame from the current one, but only with the relevant columns. Which columns are relevant is left up to you, but make sure you have at least the columns that contain the actual tweets and the geometry! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create a new dataframe called \"new_geodata\" with only the relevant columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Filter the data by location"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/nyc.PNG?raw=true\" align=\"right\">\n",
    "\n",
    "Now that we have a geodataframe, we are almost ready to go. The last preparation step will be to <b>filter</b> our data for only the relevant data that we are interested in. \n",
    "\n",
    "Specifically, we are interested only in a small area: <b>New York City</b>. \n",
    "\n",
    "To filter the data by a certain area, we need a shapefile of that area.  Take a look <u>[here](https://geodata.lib.utexas.edu/catalog/nyu-2451-34490)</u> at the shapefile we will be using.\n",
    "\n",
    "Download the shapefile from the link provided above. Store <u>all</u> of the files you have downloaded in a new folder in this repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- SOLUTION -------------------------------\n",
    "\n",
    "# Read in the boundary of New York City\n",
    "nyc = gpd.read_file(\"new york/nyc.shp\")\n",
    "\n",
    "# Reproject nyc to the common CRS\n",
    "nyc = nyc.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- SOLUTION -------------------------------\n",
    "\n",
    "# Filter your geodataframe by the boundary of New York City\n",
    "filtered_gdf = gpd.overlay(new_geodata, nyc, how='intersection')\n",
    "\n",
    "print(len(filtered_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- SOLUTION -------------------------------\n",
    "\n",
    "# View the filtered geodataframe\n",
    "filtered_gdf.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to make sure that the points in our filtered dataset are indeed in New York! We can do this using the [plotly scatter_mapbox tools](https://plotly.github.io/plotly.py-docs/generated/plotly.express.scatter_mapbox.html). Plotly maps require individual columns for latitude and longitude in order to plot the points on a map. So we'll first have to create those two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new columns for latitude and longitude\n",
    "filtered_gdf['lat'] = filtered_gdf['geometry'].apply(lambda x : x.y if x else np.nan)\n",
    "filtered_gdf['lon'] = filtered_gdf['geometry'].apply(lambda x : x.x if x else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly mapbox\n",
    "fig = px.scatter_mapbox(filtered_gdf,                       # the dataset\n",
    "                        lat=\"lat\",                          # the column in the dataset indicating the latitude\n",
    "                        lon=\"lon\",                          # the column in the dataset indicating the longitude\n",
    "                        color=\"bcode\",                      # the column in the dataset by which the points should be colored\n",
    "                        center=dict(                        # the coordinates that the map should center on\n",
    "                                    lat=40.7,\n",
    "                                    lon=-73.9\n",
    "                                ),\n",
    "                        zoom=9,                             # the initial zoom-level of the map (higher numbers = more zoomed in)\n",
    "                        mapbox_style='carto-positron',      # the style of the base map \n",
    "                        height=600                          # height of the map figure in pixels\n",
    "                        )      \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a choropleth map using a GeoDataFrame\n",
    "fig = px.choropleth_mapbox(\n",
    "    filtered_gdf,  # GeoDataFrame\n",
    "    geojson=filtered_gdf.geometry,  # Use the \"geometry\" column for mapping\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    color=\"date\",\n",
    "    zoom=3,\n",
    "    center={\"lat\": 40, \"lon\": -73},  # Set the initial center of the map\n",
    ")\n",
    "\n",
    "# Show the map\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Choose an appropriate NLP Model for your Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to think about your semantic analysis. \n",
    "\n",
    "First you may want to ask yourselves <i>\"What questions <b>can</b> we even answer using NLP?\"</i> and <i>\"What questions would be <b>useful and interesting</b> to answer?\"</i> \n",
    "\n",
    "You may browse the available models on the hugging face platform (focus on the models under the category \"Natural Language Processing\" and \"Text Classification\"). Most of the models have some documentation and descriptions. Make a decision together about which model you will use. In your submission, add a markdown cell describing your thoughts and reasons for the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Your choice of NLP model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Complete this markdown cell with your answers:</span>\n",
    "\n",
    "1. Model chosen:\n",
    "\n",
    "2. Discussion on why this particular model was chosen: \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Import the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Run a semantic analysis on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Plot the Analysis Results on a Map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Interpret the Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lesson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
